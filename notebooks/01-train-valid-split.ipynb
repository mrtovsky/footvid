{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset split\n",
    "\n",
    "The goal of this notebook is to create **train** and **validation** sets and check their consistency.\n",
    "\n",
    "**NOTE**: To display this notebook properly [Nbextensions](https://jupyter-contrib-nbextensions.readthedocs.io/en/latest/install.html) should be present and `Python Markdown` activated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T01:32:22.821022Z",
     "start_time": "2020-09-22T01:32:22.799671Z"
    }
   },
   "outputs": [],
   "source": [
    "from footvid.utils.env import check_repository_path\n",
    "\n",
    "\n",
    "REPOSITORY_DIR = check_repository_path()\n",
    "RAW_DATA_DIR = REPOSITORY_DIR.joinpath(\"data\", \"raw\")\n",
    "PROCESSED_DATA_DIR = REPOSITORY_DIR.joinpath(\"data\", \"processed\")\n",
    "\n",
    "N_POS = 1643\n",
    "N_NEG = 1335\n",
    "N_TOTAL = N_POS + N_NEG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we know from the [previous notebook](https://github.com/mrtovsky/footvid/blob/master/notebooks/00-images-integrity.ipynb) whole dataset contains {{ N_POS }} and {{ N_NEG }} negative observations. By manual investigation it has been discovered that the intersection of sets of matches contained in the `FrameFilter-set1_4k`, `FrameFilter-set2_fifawc2018` and the `FrameFilter-set2_fifawc2018` is not empty. Those folders will be merged together and among the images inside two sets will be created but in such a way that the frames from the matches included in the training set are not included in the validation set as well, what can potentially lead to a data leakage. `match_hash` field comes with help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T01:32:23.791257Z",
     "start_time": "2020-09-22T01:32:23.738803Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`match_hash` counted occurrences by classes:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(collections.Counter,\n",
       "            {'0989f5f192a285762fa1bd04': Counter({'n_pos': 39, 'n_neg': 45}),\n",
       "             'd065ce1ce94a49804498e1f0': Counter({'n_pos': 5, 'n_neg': 17}),\n",
       "             'bf8ccbeb6c3636c6b857af94': Counter({'n_pos': 46, 'n_neg': 42}),\n",
       "             'e454a13b26a10fe08ef05181': Counter({'n_pos': 16, 'n_neg': 24}),\n",
       "             'ee2c6f24bd9845218577df88': Counter({'n_pos': 34, 'n_neg': 24}),\n",
       "             '7e9a7a63ca577c8d974622fd': Counter({'n_pos': 22, 'n_neg': 17}),\n",
       "             '8feb9679c3d7e2f8f95fc179': Counter({'n_pos': 26, 'n_neg': 27}),\n",
       "             '58cf64110dc8c1634cb69e94': Counter({'n_pos': 18, 'n_neg': 26}),\n",
       "             '2a2f6aebee678fa1b8de5e77': Counter({'n_pos': 23, 'n_neg': 30}),\n",
       "             '3e7265e32a67a480085ae3e9': Counter({'n_pos': 46, 'n_neg': 40}),\n",
       "             'a09951a67b79d2a6a79c2110': Counter({'n_pos': 26, 'n_neg': 22}),\n",
       "             'ff16070272e1ab1ce5615d62': Counter({'n_pos': 20, 'n_neg': 32}),\n",
       "             'cda0284e800d20775861998a': Counter({'n_pos': 22, 'n_neg': 32}),\n",
       "             '9362066593a9f85b7b9dea1d': Counter({'n_pos': 14, 'n_neg': 16}),\n",
       "             '09eada874fc784a668315070': Counter({'n_pos': 21, 'n_neg': 39}),\n",
       "             'b02b6c3d281910efa015269b': Counter({'n_pos': 32, 'n_neg': 34}),\n",
       "             '2c84c37bbae8d0dac63ce40d': Counter({'n_pos': 7, 'n_neg': 15}),\n",
       "             'ccf0b82b884d4ed34190d632': Counter({'n_pos': 110, 'n_neg': 63}),\n",
       "             'acf5ee0855aa27bf6c32b68f': Counter({'n_pos': 137, 'n_neg': 47}),\n",
       "             'b1c5ab9677f4531b3fd81091': Counter({'n_pos': 343, 'n_neg': 336}),\n",
       "             '0d6487449b59da08d5ea4641': Counter({'n_pos': 130, 'n_neg': 81}),\n",
       "             'f2c70f8a579c472e716785c3': Counter({'n_pos': 133, 'n_neg': 73}),\n",
       "             '4d8ae253129646ce044af9f6': Counter({'n_pos': 126, 'n_neg': 78}),\n",
       "             '239cfe735df8fbe4597e4a43': Counter({'n_pos': 119, 'n_neg': 98}),\n",
       "             'b84576ba0a22bfb909fbed0c': Counter({'n_pos': 128, 'n_neg': 77})})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import Counter, defaultdict\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "# Store paths to image files.\n",
    "pos_image_files = []\n",
    "neg_image_files = []\n",
    "\n",
    "for data_folder in sorted(RAW_DATA_DIR.glob(\"*\")):\n",
    "    if data_folder.is_dir():\n",
    "        for label_folder in data_folder.glob(\"*\"):\n",
    "            if label_folder.is_dir():\n",
    "                if label_folder.name == \"neg\":\n",
    "                    neg_image_files.extend(list(label_folder.glob(\"*\")))\n",
    "                elif label_folder.name == \"pos\":\n",
    "                    pos_image_files.extend(list(label_folder.glob(\"*\")))\n",
    "\n",
    "match_hash_counter = defaultdict(Counter)\n",
    "for file in pos_image_files:\n",
    "    match_hash_counter[file.stem.split(\"-\")[1]][\"n_pos\"] += 1\n",
    "for file in neg_image_files:\n",
    "    match_hash_counter[file.stem.split(\"-\")[1]][\"n_neg\"] += 1\n",
    "\n",
    "print(\"`match_hash` counted occurrences by classes:\\n\")\n",
    "display(match_hash_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's all about selecting the proper $train : valid$ proportion and dividing frames into train and validation set by searching the combination that is closer to the proportions specified. $7 : 3$ seems to be a reasonable split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T01:32:25.439903Z",
     "start_time": "2020-09-22T01:32:25.433648Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theoretical splits are characterized by the numbers below:\n",
      "* TRAIN: 1150 positives, 934 negatives.\n",
      "* VALID: 492 positives, 400 negatives.\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "DatasetSize = namedtuple(\"DatasetSize\", [\"n_pos\", \"n_neg\"])\n",
    "\n",
    "pos_percentage = N_POS / (N_POS + N_NEG)\n",
    "theoretical_train_size = DatasetSize(\n",
    "    n_pos=int(N_TOTAL * pos_percentage * 0.7),\n",
    "    n_neg=int(N_TOTAL * (1 - pos_percentage) * 0.7),\n",
    ")\n",
    "theoretical_valid_size = DatasetSize(\n",
    "    n_pos=int(N_TOTAL * pos_percentage * 0.3),\n",
    "    n_neg=int(N_TOTAL * (1 - pos_percentage) * 0.3),\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"Theoretical splits are characterized by the numbers below:\\n\"\n",
    "    f\"* TRAIN: {theoretical_train_size.n_pos} positives, {theoretical_train_size.n_neg} negatives.\\n\"\n",
    "    f\"* VALID: {theoretical_valid_size.n_pos} positives, {theoretical_valid_size.n_neg} negatives.\"\n",
    ")\n",
    "keys = list(match_hash_counter.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** Paragraph below contains LateX equation that may not render properly on GitHub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-21T23:36:45.487040Z",
     "start_time": "2020-09-21T23:36:45.482905Z"
    },
    "variables": {
     " len(keys) ": "25"
    }
   },
   "source": [
    "Knowing the aforementioned numbers will let us optimize the division of frames across train and valid sets by, in an ideal world, considering all of the possible combinations of dividing matches by hashes and minimizing the following loss:\n",
    "\n",
    "$$L := \\sum_{\\textrm{c, d } \\in \\textbf{ C } \\times \\textbf{ D}}{(actual_{c,d} - theoretical_{c, d})^2}$$\n",
    "$$\\textrm{where: }$$\n",
    "$$\\textbf{C} = \\{\\textrm{positive, negative}\\} \\textrm{,}$$\n",
    "$$\\textbf{D} = \\{\\textrm{train, valid}\\}\\textrm{.}$$\n",
    "\n",
    "In general, we want to find the split that is closest to the calculated theoretical splits, by penalizing greater deviations from the theoretical values harder.\n",
    "\n",
    "Due to the fact that the number of unique matches is {{ len(keys) }} it gives more than 33 million combinations - $2^{25}$. Thus, we will apply an algorithm that works with the $O(n^2)$ worst-case time complexity. First, we will sort hashes by a total size increasingly and then we will include one hash after another until the theoretical shape is exceeded. Then we will take a n-steps back and check if replacing last-n hashes with the current one will improve the **validation set** proportions - this is another simplification, we will only try to optimize the validation set proportions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T01:32:26.822864Z",
     "start_time": "2020-09-22T01:32:26.810531Z"
    }
   },
   "outputs": [],
   "source": [
    "MatchHash = namedtuple(\"MatchHash\", [\"match_hash\", \"n_pos\", \"n_neg\", \"n_total\"])\n",
    "\n",
    "match_hashes = [\n",
    "    MatchHash(\n",
    "        match_hash=key,\n",
    "        n_pos=value[\"n_pos\"],\n",
    "        n_neg=value[\"n_neg\"],\n",
    "        n_total=value[\"n_pos\"] + value[\"n_neg\"],\n",
    "    )\n",
    "    for key, value in match_hash_counter.items()\n",
    "] \n",
    "\n",
    "match_hashes.sort(key=lambda tup: tup[-1])\n",
    "\n",
    "\n",
    "def calculate_loss(\n",
    "    match_hashes,\n",
    "    theoretical_n_pos=theoretical_valid_size.n_pos,\n",
    "    theoretical_n_neg=theoretical_valid_size.n_neg,\n",
    "):\n",
    "    n_pos_sum = sum([match_hash.n_pos for match_hash in match_hashes])\n",
    "    n_neg_sum = sum([match_hash.n_neg for match_hash in match_hashes])\n",
    "    \n",
    "    return (theoretical_n_pos - n_pos_sum) ** 2 + (theoretical_n_neg - n_neg_sum) ** 2\n",
    "\n",
    "\n",
    "best_valid_hashes = [match_hashes[0],]\n",
    "loss = calculate_loss(best_valid_hashes)\n",
    "for match_hash in match_hashes[1:]:\n",
    "    best_hashes_n_pos_sum = sum([best_hash.n_pos for best_hash in best_valid_hashes])\n",
    "    best_hashes_n_neg_sum = sum([best_hash.n_neg for best_hash in best_valid_hashes])\n",
    "    if (\n",
    "        best_hashes_n_pos_sum < theoretical_valid_size.n_pos\n",
    "        and best_hashes_n_neg_sum < theoretical_valid_size.n_neg\n",
    "    ):\n",
    "        best_valid_hashes.append(match_hash)\n",
    "        loss = calculate_loss(best_valid_hashes)\n",
    "    else:\n",
    "        challenger_best_loss = 9e9\n",
    "        challenger_best_idx = None\n",
    "        for idx in range(1, len(best_valid_hashes)):\n",
    "            challenger_valid_hashes = best_valid_hashes[:-idx] + [match_hash,]\n",
    "            challenger_loss = calculate_loss(challenger_valid_hashes)\n",
    "            if challenger_loss < loss and challenger_loss < challenger_best_loss:\n",
    "                challenger_best_loss = challenger_loss\n",
    "                challenger_best_idx = idx\n",
    "        if challenger_best_idx is not None:\n",
    "            best_valid_hashes = best_valid_hashes[:-challenger_best_idx] + [match_hash,]\n",
    "            loss = challenger_best_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This way we have extracted the most suitable combination of match hashes to construct the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T01:32:27.841298Z",
     "start_time": "2020-09-22T01:32:27.837241Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The validation set is characterized by the following numbers:\n",
      "* positives: 469,\n",
      "* negatives: 442,\n",
      "* total: 911.\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"The validation set is characterized by the following numbers:\\n\"\n",
    "    f\"* positives: {sum([best_hash.n_pos for best_hash in best_valid_hashes])},\\n\"\n",
    "    f\"* negatives: {sum([best_hash.n_neg for best_hash in best_valid_hashes])},\\n\"\n",
    "    f\"* total: {sum([best_hash.n_total for best_hash in best_valid_hashes])}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those numbers are pretty close to the theoretical values, so we can assume that it is a fair division."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Files segregation\n",
    "\n",
    "When the match hashes that make up the validation set are known it is time to properly segregate raw files into **train** and **valid** folders. Those folders should be a subdirectory of the **processed** directory and the structure should look as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T01:32:29.170199Z",
     "start_time": "2020-09-22T01:32:29.019578Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m../data/processed/\u001b[00m\r\n",
      "├── \u001b[01;34mtrain\u001b[00m\r\n",
      "│   ├── \u001b[01;34mneg\u001b[00m\r\n",
      "│   └── \u001b[01;34mpos\u001b[00m\r\n",
      "└── \u001b[01;34mvalid\u001b[00m\r\n",
      "    ├── \u001b[01;34mneg\u001b[00m\r\n",
      "    └── \u001b[01;34mpos\u001b[00m\r\n",
      "\r\n",
      "6 directories\r\n"
     ]
    }
   ],
   "source": [
    "!tree -d ../data/processed/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T01:35:07.750358Z",
     "start_time": "2020-09-22T01:34:51.577170Z"
    }
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "\n",
    "valid_match_hashes = set([valid_hash.match_hash for valid_hash in best_valid_hashes])\n",
    "\n",
    "for file in pos_image_files:\n",
    "    if file.stem.split(\"-\")[1] in valid_match_hashes:\n",
    "        shutil.copy(file, PROCESSED_DATA_DIR.joinpath(\"valid\", \"pos\"))\n",
    "    else:\n",
    "        shutil.copy(file, PROCESSED_DATA_DIR.joinpath(\"train\", \"pos\"))\n",
    "\n",
    "for file in neg_image_files:\n",
    "    if file.stem.split(\"-\")[1] in valid_match_hashes:\n",
    "        shutil.copy(file, PROCESSED_DATA_DIR.joinpath(\"valid\", \"neg\"))\n",
    "    else:\n",
    "        shutil.copy(file, PROCESSED_DATA_DIR.joinpath(\"train\", \"neg\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "footvid-venv",
   "language": "python",
   "name": "footvid-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
